{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome Everyone\n",
    "\n",
    "This Notebook will be helpful to understand how to liverage the AI to answer from our own documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **UNSTRUCTURED Library**\n",
    "The unstructured library is designed to help preprocess structure unstructured text documents for use in downstream machine learning tasks. Examples of documents that can be processed using the unstructured library include PDFs, XML and HTML documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LOADING THE DOCUMENT(file)**\n",
    "Plain text files, HTML, XML, JSON, and Emails are immediately supported without any additional dependencies.\n",
    "\n",
    "If youâ€™re processing document types beyond the basics, you can install the necessary extras like: \n",
    "\n",
    "**pip install \"unstructured[docx,pdf]\"**\n",
    "\n",
    "To install all the additional document types:\n",
    "\n",
    "**pip install \"unstructured[all-docs]\"**\n",
    "\n",
    "\"csv\", \"doc\", \"docx\", \"epub\", \"image\", \"md\", \"msg\", \"odt\", \"org\", \"pdf\", \"ppt\", \"pptx\", \"rtf\", \"rst\", \"tsv\", \"xlsx\"\n",
    "\n",
    "Note: We installed few document extensions already for you to try\n",
    "(docx,pdf,pptx,md,msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "folder_path = \"docs\"\n",
    "# Using glob to get a list of file paths in the specified folder and its subfolders\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"**\"), recursive=True)\n",
    "\n",
    "file_elements = []\n",
    "for file_path in file_paths:\n",
    "    if os.path.isfile(file_path):\n",
    "        elements = partition(filename=file_path)\n",
    "        file_elements.append(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How the Documents(file) are categorized into elements**\n",
    "\n",
    "The element objects represent different components of the source document. Examples: NarrativeText , ListItem, Title, Table, Image, etc..\n",
    "\n",
    "To know more about elements visit: https://unstructured-io.github.io/unstructured/introduction.html#document-elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total element count: 3984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'Title',\n",
       " 'element_id': '12266cc0b8b0d8278150b03a8bc457cf',\n",
       " 'metadata': {'coordinates': {'points': ((66.767, 52.48632479999992),\n",
       "    (66.767, 104.29162479999991),\n",
       "    (545.2360133, 104.29162479999991),\n",
       "    (545.2360133, 52.48632479999992)),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 612,\n",
       "   'layout_height': 792},\n",
       "  'filename': 'IEEE.pdf',\n",
       "  'file_directory': 'docs',\n",
       "  'last_modified': '2023-09-28T10:34:08',\n",
       "  'filetype': 'application/pdf',\n",
       "  'page_number': 1},\n",
       " 'text': 'CNN based Curved Path Detection and Obstacle Avoidance for Autonomous Rover'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_elements = sum(len(row) for row in file_elements)\n",
    "print(f\"Total element count: {total_elements}\")\n",
    "file_elements[0][0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chunking the documents**\n",
    "Document Chunking is process of converting the document into pages or paragraphs or sentences.\n",
    "\n",
    "Here unstructured package providing method to chunk the documents into paragraphs based on the Title's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(chunks) = 429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'CompositeElement',\n",
       " 'element_id': 'c1815235350bbd9718006fb5f4aeecad',\n",
       " 'metadata': {'filename': 'IEEE.pdf',\n",
       "  'file_directory': 'docs',\n",
       "  'last_modified': '2023-09-28T10:34:08',\n",
       "  'filetype': 'application/pdf',\n",
       "  'page_number': 1},\n",
       " 'text': 'CNN based Curved Path Detection and Obstacle Avoidance for Autonomous Rover\\n\\nSudeep Aryan G 1, Bhanu Meher Srinavas R 2 ,Neethika K 3,Dr.Jayabarathi R 4 Department of Electrical and Electronics Engineering Amrita School of Engineering,Coimbatore Amrita Vishwa Vidyapeetham, India cb.en.u4eee19151@cb.students.amrita.edu, r jayabarathi@cb.amrita.edu'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "chunks = []\n",
    "for elements in file_elements:\n",
    "    chunks.extend(chunk_by_title(elements))\n",
    "\n",
    "print(f\"{len(chunks) = }\")\n",
    "chunks[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initializing the Large Language Models**\n",
    "Choosing the model for embedding and summarizing the results from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load secrets from .env file\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "llm = OpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
    "embedding_llm = \"local:BAAI/bge-large-en\"\n",
    "embedding_llm = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embedding the chunks**\n",
    "Converting the Chunks into Llama-index Document objects and embedding those objects\n",
    "\n",
    "Here we are using the default VectorStore from the Llama-Index. Few other VectorDBs availble are: **Pinecone, Chroma, Faiss** and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, Document, ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_llm)\n",
    "\n",
    "# Converting ech chunk into llama-index's Document\n",
    "documents = list(\n",
    "    map(\n",
    "        lambda chunk: Document(text=chunk.text, extra_info=chunk.metadata.to_dict()),\n",
    "        chunks,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Embedding the Documents(Chunks)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storing the Embedding**\n",
    "\n",
    "Embedding are the nummerical representation of the tokens. Tokens can be a word or piece of word.\n",
    "\n",
    "Instead of embed the same document for multiple times. Embedding can be stored in locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\n",
    "    persist_dir=\"VectorStore-online\"\n",
    ")  # persist_dir is the folder that the embeddings will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the Embedding**\n",
    "\n",
    "Loading the embedding from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./VectorStore-online\")\n",
    "loaded_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import MetadataFilters, ExactMatchFilter\n",
    "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"filename\", value=\"tps25750.pdf\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a query engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_SIMILARITIES = (\n",
    "    5  # How many Chunks needs to be retrieved, for each query from the user\n",
    ")\n",
    "query_engine = index.as_query_engine(similarity_top_k=TOP_SIMILARITIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quering the Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot provide the abstract of this IEEE paper based on the given context information.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"what is the abstract of this IEEE paper CNN\"\n",
    ")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sources Nodes Retrived for the User Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'IEEE.pdf',\n",
       " 'file_directory': 'docs',\n",
       " 'last_modified': '2023-09-28T10:34:08',\n",
       " 'filetype': 'application/pdf',\n",
       " 'page_number': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = [node for node in response.source_nodes]\n",
    "# print(*nodes, sep=\"\\n\")\n",
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample UI to try Q&A on your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_response(input_text):\n",
    "    response = query_engine.query(input_text)\n",
    "    return response.response\n",
    "\n",
    "\n",
    "question_text = gr.Textbox(lines=7, label=\"Question\")\n",
    "answer_text = gr.Textbox(lines=7, label=\"Answer\")\n",
    "\n",
    "gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=question_text,\n",
    "    outputs=answer_text,\n",
    "    title=\"Q&A on your Documents\",\n",
    ").launch(inline=False, inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
